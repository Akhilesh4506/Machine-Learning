# -*- coding: utf-8 -*-
"""Linear Regression with one variable

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-e9qlqWarehRCLgTg3zlzKqN9GwWyyST
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from google.colab import files
uploaded=files.upload()
df=pd.read_csv("ex1data1.txt",usecols=[0,1],names=["population","profit"])

df

df.describe()

x=df.iloc[:,0]
y=df.iloc[:,1]
m=len(x)

plt.figure(figsize=(8,6))
plt.scatter(x,y,c='g',marker='x',s=50, alpha=0.7)
plt.xlabel("Population of City in 10,000s")
plt.ylabel("Profit in $10,000s")
plt.show()

"""### **1)  Linear Regression Algorithm(Gradient Descent) For Fitting a Straight Line**"""

def gradient_decent(alpha,n_itr):
  x=df["population"].values
  y=df["profit"].values
  m=len(x);
  theta0=0
  theta1=0
  for _ in range(n_itr):
    d_theta0=[]
    d_theta1=[]
    for i in range(m):
      d_theta0.append((theta0 + theta1*x[i])-y[i])
      d_theta1.append(((theta0 + theta1*x[i])-y[i])*x[i])

    theta0=theta0-alpha*(1/m)*sum(d_theta0)
    theta1=theta1-alpha*(1/m)*sum(d_theta1)

  return theta0,theta1
  
theta0,theta1=gradient_decent(0.01,1500)
print('intercept_term(theta0):',theta0,'\n','bias_term(theta1):',theta1)

# Commented out IPython magic to ensure Python compatibility.
hypothesis = theta0+np.dot(theta1,[5,22.5])
x=[5,22.5]
# %matplotlib inline
plt.figure(figsize=(8,6))
plt.scatter(df['population'],df['profit'], c='g',marker='x',s=50, alpha=0.7);
plt.plot(x,hypothesis, c='r')
plt.xlabel("Population of City in 10,000s",size=13)
plt.ylabel("Profit in $10,000s",size=13)
plt.plot()
#plt.xlim(0,25)
#plt.ylim(-5,25)

def predict(X):
    return theta0 + theta1*X
predict(5)

"""### **2) Linear Regression Model Using Train Test Split**"""

from sklearn.model_selection import train_test_split;
from sklearn.linear_model import LinearRegression;

x=df["population"].values
y=df["profit"].values
#x=df.iloc[:,0]
#y=df.iloc[:,1]

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=100)
lr=LinearRegression()

y_test_pred=predict(x_test)
df2=pd.DataFrame({'Actual':y_test,'Predicted':y_test_pred})
df2.head()

#We need to give both the fit and predict methods 2D arrays. our x_train, y_train and x_test are currently only 1D
x_train=x_train.reshape(-1,1)
y_train=y_train.reshape(-1,1)
x_test=x_test.reshape(1,-1)
lr.fit(x_train,y_train)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
plt.figure(figsize=(8,6))
plt.scatter(x,y, c='g',marker='x',s=50, alpha=0.7);
plt.plot(x,y_predict, c='r')
plt.xlabel("Population of City in 10,000s",size=13)
plt.ylabel("Profit in $10,000s",size=13)
plt.plot()

intr=lr.intercept_;
bias=lr.coef_
print('intercept_term(theta0):',intr,'\n','bias_term(theta1):',bias)

from sklearn import metrics;

meanAbsErr=metrics.mean_absolute_error(y_test,y_test_pred)
meanSqErr=metrics.mean_squared_error(y_test,y_test_pred)
rootMeanSqErr=np.sqrt(meanSqErr)
print("Mean Absolute Error: ",meanAbsErr)
print("Mean Squared Error: ",meanSqErr)
print("Root Mean Squared Error: ",rootMeanSqErr)